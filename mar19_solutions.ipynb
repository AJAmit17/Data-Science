{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Min-max scaling is a data preprocessing technique used to rescale data in the range between 0 and 1, where the minimum value is mapped to 0 and the maximum value is mapped to 1. It preserves the relative differences between different values in the data.\n",
    "\n",
    "In order to apply min-max scaling, we subtract the minimum value from each data point and then divide it the range of the data. The general formula for min-max scaling is:\n",
    "\n",
    "x_scaled = (x - min(x)) / (max(x) - min(x))\n",
    "\n",
    "where x is a data point in the dataset, and x_scaled is the corresponding scaled value.\n",
    "\n",
    "For example, let's say we have a dataset of salaries of employees in a company, ranging from $30,000 to $70,000. We can use min-max scaling to rescale the data to a range of 0 to 1 by applying the following formula:\n",
    "\n",
    "salary_scaled = (salary - 30,000) / (70,000 - 30,000)\n",
    "\n",
    "If an employee's salary is $50,000, the corresponding scaled value would be:\n",
    "\n",
    "salary_scaled = (50,000 - 30,000) / (70,000 - 30,000) = 0.5\n",
    "\n",
    "Using min-max scaling can make it easier to compare different features with different scales, since all features will be on the same scale."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Unit Vector technique, also known as normalization, is a feature scaling method where each data point is transformed into a vector with a length of 1.\n",
    "\n",
    "To apply Unit Vector technique, we divide each value in a feature by the Euclidean norm of the feature. The Euclidean norm is the square root of the sum of the squares of all the values in the feature.\n",
    "\n",
    "For example, suppose we have a dataset that contains a feature X with values [2, 4, 6]. To normalize this feature using Unit Vector technique, we first calculate the Euclidean norm of X:\n",
    "\n",
    "sqrt(2^2 + 4^2 + 6^2) = 7.48\n",
    "\n",
    "Then, we divide each value in X by the Euclidean norm:\n",
    "\n",
    "[2/7.48, 4/7.48, 6/7.48] = [0.27, 0.54, 0.81]\n",
    "\n",
    "The resulting vector has a length of 1, which means it lies on the surface of a unit sphere.\n",
    "\n",
    "The main difference between Unit Vector technique and Min-Max scaling is that Min-Max scaling scales the data to a fixed range (usually between 0 and 1), whereas Unit Vector technique uses the magnitude of the vector to perform scaling.\n",
    "\n",
    "For example, suppose we have a dataset that contains a feature X with values [2, 4, 6]. To scale this feature using Min-Max scaling, we first calculate the minimum and maximum values of X:\n",
    "\n",
    "min(X) = 2\n",
    "max(X) = 6\n",
    "\n",
    "Then, we apply the Min-Max formula to each value in X:\n",
    "\n",
    "[(2-2)/(6-2), (4-2)/(6-2), (6-2)/(6-2)] = [0, 0.5, 1]\n",
    "\n",
    "The resulting vector has values in a fixed range between 0 and 1.\n",
    "\n",
    "In summary, Unit Vector technique scales the data based on its magnitude, while Min-Max scaling scales the data to a fixed range."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "PCA (Principle Component Analysis) is a statistical technique that reduces the dimensionality of a dataset by identifying a smaller number of uncorrelated variables, known as the principal components, which explain the maximum variance in the original dataset.\n",
    "\n",
    "PCA is used in dimensionality reduction to address the curse of dimensionality, where high-dimensional datasets can have many features and be difficult to analyze, display, or model. PCA helps to simplify complex datasets by transforming them into a new coordinate system, where the principal components are used as the new features and represent the underlying patterns in the data.\n",
    "\n",
    "An example of PCA in action could be in analyzing a dataset of stock prices for various companies. Suppose the dataset contains 10 years of daily stock prices for 500 companies, resulting in a 500x3,650 matrix (500 rows for each company and 3,650 columns for each day). This dataset is high-dimensional and difficult to analyze.\n",
    "\n",
    "By applying PCA, we can reduce the dimensionality of the dataset by extracting the principal components, which represent the most significant variations in the stock prices. The PCA algorithm will create a new set of features that linearly combine the original features and produce a lower-dimension representation of the data.\n",
    "\n",
    "For example, the first principal component might represent the overall market trend, and the second component might represent the variation in stock prices due to industry-specific factors. By visualizing the data in this reduced dimensionality space, we can gain insights into patterns and trends that would be challenging to identify in the original dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "PCA is a technique for reducing data dimensionality while retaining its important characteristics. Feature extraction is a type of dimensionality reduction in which an algorithm selects the most important features from a dataset. PCA can be used for feature extraction by identifying the most important features through the creation of principal components.\n",
    "\n",
    "For example, consider a dataset of images containing thousands of pixels. PCA can be used to identify the principal components that capture the most important patterns in the images. These principal components can then be used as new, reduced features to represent each image, rather than using all of the original pixels. This reduces the size of the dataset and simplifies subsequent analyses while still retaining most of the important information from the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "To use Min-Max scaling for preprocessing the data, we would first determine the range of each feature by finding the minimum and maximum value of each feature in the dataset. After that, we would apply the Min-Max scaling formula to scale the values of each feature within a given range, usually between 0 and 1. \n",
    "\n",
    "The Min-Max scaling formula is as follows:\n",
    "\n",
    "scaled_value = (value - min_value) / (max_value - min_value)\n",
    "\n",
    "For example, suppose we have the feature 'price' in our dataset, with a minimum value of $5 and a maximum value of $15. To apply Min-Max scaling to this feature, we would subtract the minimum value ($5) from each value of the feature and divide it by the range between the maximum and minimum value, that is ($15 - $5 = $10). So, for a price of $7, the Min-Max scaled value would be:\n",
    "\n",
    "scaled_value = ($7 - $5) / ($15 - $5) = 0.2\n",
    "\n",
    "We would perform the same operation for each feature in the dataset using their respective minimum and maximum values. This scaling technique ensures that all features are on the same scale and reduces the impact of features that have a large range on the final recommendation results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "PCA (Principal Component Analysis) is a commonly used technique for dimensionality reduction in datasets with many features. \n",
    "\n",
    "In the case of predicting stock prices with a large dataset containing many features, the first step is to identify the features that are most important in predicting stock prices. PCA can help with this task by identifying the most significant features that explain most of the variance in the dataset.\n",
    "\n",
    "The steps involved in using PCA for dimensionality reduction in this case are as follows:\n",
    "\n",
    "1. Standardize the Features: Before applying PCA, it is necessary to standardize the dataset by subtracting the mean and dividing by standard deviation to make them comparable.\n",
    "\n",
    "2. Calculate covariance Matrix: The next step is to calculate the covariance matrix of the standardized features.\n",
    "\n",
    "3. Calculate Eigenvectors and Eigenvalues: The next step is to calculate the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the principal components, while the eigenvalues represent the amount of variance explained by each principal component.\n",
    "\n",
    "4. Select the number of principal components: Decide on the number of principal components to keep, based on the percentage of variance each component explains. A common rule is to keep enough principal components to explain at least 70-80% of the variance in the dataset.\n",
    "\n",
    "5. Transform the Data: Finally, transform the original dataset using the selected principal components to obtain a reduced dimensionality dataset.\n",
    "\n",
    "In conclusion, PCA can be used to select the most important features and reduce the dimensionality of a large dataset of stock data, making it easier to build an accurate predictive model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "To perform Min-Max scaling on the dataset, we can use the following formula:\n",
    "\n",
    "X_scaled = (X - X_min) / (X_max - X_min) * (max_range - min_range) + min_range\n",
    "\n",
    "where X is the original value, X_min is the minimum value in the dataset, X_max is the maximum value in the dataset, max_range is the maximum range we want to scale to (in this case, 1), and min_range is the minimum range we want to scale to (in this case, -1).\n",
    "\n",
    "Using this formula, we can transform the values in the dataset as follows:\n",
    "\n",
    "X_min = 1\n",
    "X_max = 20\n",
    "\n",
    "X_scaled = (X - X_min) / (X_max - X_min) * (1 - (-1)) + (-1)\n",
    "\n",
    "For X=1:\n",
    "X_scaled = (1 - 1) / (20 - 1) * (1 - (-1)) + (-1) = -1\n",
    "\n",
    "For X=5:\n",
    "X_scaled = (5 - 1) / (20 - 1) * (1 - (-1)) + (-1) = -0.6\n",
    "\n",
    "For X=10:\n",
    "X_scaled = (10 - 1) / (20 - 1) * (1 - (-1)) + (-1) = -0.2\n",
    "\n",
    "For X=15:\n",
    "X_scaled = (15 - 1) / (20 - 1) * (1 - (-1)) + (-1) = 0.2\n",
    "\n",
    "For X=20:\n",
    "X_scaled = (20 - 1) / (20 - 1) * (1 - (-1)) + (-1) = 1\n",
    "\n",
    "Therefore, the Min-Max scaled values for the dataset [1, 5, 10, 15, 20] in the range of -1 to 1 are [-1, -0.6, -0.2, 0.2, 1]."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "To perform feature extraction using PCA on this dataset, we would first need to standardize the features since they are measured on different scales. Then we would calculate the covariance matrix of the standardized features and obtain the eigenvectors and eigenvalues.\n",
    "\n",
    "Based on the eigenvalues, we would choose to retain as many principal components as necessary to explain at least 80-90% of the variance in the data. We can plot the cumulative sum of the explained variance against the number of principal components and choose the minimum number of components that explain 80-90% of the variance.\n",
    "\n",
    "The specific number of principal components to retain would depend on the dataset and the level of variance we want to explain. In general, we would aim to retain fewer components to reduce the dimensionality of the data while still retaining most of the information. In this case, we might expect that height, weight, and age would be the most important features for predicting blood pressure, so we may choose to retain 2-3 principal components that capture the majority of the variance in these features."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
